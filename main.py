import os
import requests
import streamlit as st
import docx2txt
import PyPDF2
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
import langdetect


# 拽   住转 Streamlit Cloud
STREAMLIT_DEPLOYMENT = os.getenv('STREAMLIT_DEPLOYMENT', 'false').lower() == 'true'

# 住  ChromeDriverManager 专拽   住转 Streamlit Cloud
if not STREAMLIT_DEPLOYMENT:
    try:
        from webdriver_manager.chrome import ChromeDriverManager
    except ImportError:
        ChromeDriverManager = None
else:
    ChromeDriverManager = None

def create_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    
   # 拽   住转 爪专  Streamlit Cloud
    if os.getenv('ENVIRONMENT') == 'production' or STREAMLIT_DEPLOYMENT:
        service = Service('/usr/bin/chromedriver')
    else:
        # 住转 驻转, 砖转砖 -ChromeDriverManager   
        if ChromeDriverManager:
            service = Service(ChromeDriverManager().install())
        else:
            #  ChromeDriverManager  , 住 砖转砖 专专 拽
            service = Service('chromedriver')
    
    return webdriver.Chrome(service=service, options=chrome_options)

# 砖专 拽 砖专  砖
import io
import pandas as pd
from dotenv import load_dotenv
import urllib.parse
from bs4 import BeautifulSoup
import json
from openpyxl import Workbook
from openpyxl.styles import Font, Alignment
import base64

load_dotenv()

def load_resume(file):
    file_type = file.type
    if file_type == "application/pdf":
        return read_pdf(file)
    elif file_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        return read_docx(file)
    else:
        st.error("驻专 拽抓  转.  注 拽抓 PDF  Word.")
        return None

def read_pdf(file):
    pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.getvalue()))
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text()
    return text

def read_docx(file):
    return docx2txt.process(io.BytesIO(file.getvalue()))

# 注转 住 拽抓 JSON
with open('agents.json', 'r', encoding='utf-8') as file:
    agents = json.load(file)

def request_url(agent, prompt, page=1):
    driver = create_driver()
    base_url = agent['url']
    start_text = agent['start_text']
    end_text = agent['end_text']
    
    encoded_prompt = urllib.parse.quote(prompt)
    if '{page}' in base_url:
        url = base_url.format(prompt=encoded_prompt, page=page)
    else:
        url = base_url.format(prompt=encoded_prompt)
    
    try:
        driver.get(url)
        content = driver.page_source
        
        start_index = content.find(start_text)
        end_index = content.find(end_text, start_index)

        if start_index != -1 and end_index != -1:
            extracted_content = content[start_index:end_index + len(end_text)]
            return extracted_content
        else:
            st.warning(f" 爪 转 转 注专 {agent['name']}")
            return None

    except Exception as e:
        st.error(f"专注 砖 注转 砖驻转 转 -{agent['name']}: {str(e)}")
        return None
    finally:
        driver.quit()

def extract_jobs_drushim(html_content, agent_name):
    soup = BeautifulSoup(html_content, 'html.parser')
    jobs = []
    
    for job_div in soup.find_all('div', class_='job-item-main'):
        job = {'source': agent_name}  # Add the agent name to each job
        
        # 转驻拽
        title = job_div.find('h3', class_='display-28')
        job['title'] = title.text.strip() if title else ''
        
        # 专
        company = job_div.find('p', class_='display-22')
        job['company'] = company.text.strip() if company else ''
        
        # 拽, 砖转 住, 拽祝 砖专, 驻专住 驻
        details = job_div.find_all('span', class_='display-18')
        for detail in details:
            text = detail.text.strip()
            if '砖' in text:
                job['experience'] = text
            elif any(word in text for word in ['砖专 ', '砖专 拽转']):
                job['job_type'] = text
            elif '驻' in text:
                job['posted'] = text
            else:
                job['location'] = text
        
        jobs.append(job)
    
    return jobs
def extract_jobs_jobmaster(html_content, agent_name):
    soup = BeautifulSoup(html_content, 'html.parser')
    jobs = []
    
    for job_div in soup.find_all('article', class_='JobItem'):
        job = {'source': agent_name}  # Add the agent name to each job
        
        # 转驻拽 (Title)
        title = job_div.find('a', class_='CardHeader')
        job['title'] = title.text.strip() if title else ''
        
        # 专 (Company)
        company = job_div.find('a', class_='CompanyNameLink') or job_div.find('span', class_='ByTitle')
        job['company'] = company.text.strip() if company else ''
        
        # 拽 (Location)
        location = job_div.find('li', class_='jobLocation')
        job['location'] = location.text.strip() if location else ''
        
        # 砖转 住 (Experience)
        # JobMaster doesn't seem to have a specific field for experience, so we'll leave it empty
        job['experience'] = ''
        
        # 拽祝 砖专 (Job Type)
        job_type = job_div.find('li', class_='jobType')
        job['job_type'] = job_type.text.strip() if job_type else ''
        
        # 驻专住 驻 (Posted)
        posted = job_div.find('span', class_='Gray')
        job['posted'] = posted.text.strip() if posted else ''
        
        # 转专 拽爪专 (Short Description)
        description = job_div.find('div', class_='jobShortDescription')
        job['description'] = description.text.strip() if description else ''
        
        jobs.append(job)
    
    return jobs
def extract_jobs_avodata(html_content, agent_name):
    soup = BeautifulSoup(html_content, 'html.parser')
    jobs = []
    
    for job_div in soup.find_all('div', class_='result-TaasukaCatalog'):
        job = {'source': agent_name}
        
        # 转驻拽 (Title)
        title = job_div.find('div', class_='title')
        job['title'] = title.text.strip() if title else ''
        
        # 转专 (Description)
        description = job_div.find('div', class_='sub-title')
        job['description'] = description.text.strip() if description else ''
        
        # 注 住祝
        info_list = job_div.find('ul')
        if info_list:
            for li in info_list.find_all('li'):
                if 'belongsToScope' in li.get('class', []):
                    job['field'] = li.text.split(': ')[1] if ': ' in li.text else ''
                elif 'salary' in li.get('class', []):
                    job['salary'] = li.text.split(': ')[1] if ': ' in li.text else ''
        
        # 拽 砖专
        link = job_div.find('a', class_='result-TaasukaCatalog')
        job['link'] = link['href'] if link and 'href' in link.attrs else ''
        
        # 砖转 砖  -Avodata
        job['company'] = ''
        job['location'] = ''
        job['experience'] = ''
        job['job_type'] = ''
        job['posted'] = ''
        
        jobs.append(job)
    
    return jobs

def create_excel_from_json(jobs):
    wb = Workbook()
    ws = wb.active
    ws.title = "专砖转 砖专转"

    headers = ['拽专', '转驻拽', '专', '拽', '住', '住 砖专', '驻专住']

    for col, header in enumerate(headers, start=1):
        cell = ws.cell(row=1, column=col, value=header)
        cell.font = Font(bold=True)
        cell.alignment = Alignment(horizontal='center')

    for row, job in enumerate(jobs, start=2):
        ws.cell(row=row, column=1, value=job.get('source', ''))
        ws.cell(row=row, column=2, value=job.get('title', ''))
        ws.cell(row=row, column=3, value=job.get('company', ''))
        ws.cell(row=row, column=4, value=job.get('location', ''))
        ws.cell(row=row, column=5, value=job.get('experience', ''))
        ws.cell(row=row, column=6, value=job.get('job_type', ''))
        ws.cell(row=row, column=7, value=job.get('posted', ''))

    for column in ws.columns:
        max_length = 0
        column_letter = column[0].column_letter
        for cell in column:
            try:
                if len(str(cell.value)) > max_length:
                    max_length = len(cell.value)
            except:
                pass
        adjusted_width = (max_length + 2)
        ws.column_dimensions[column_letter].width = adjusted_width

    return wb

def get_table_download_link(df):
    """Generates a link allowing the data in a given panda dataframe to be downloaded"""
    output = io.BytesIO()
    writer = pd.ExcelWriter(output, engine='openpyxl')
    df.to_excel(writer, index=False, sheet_name='Sheet1')
    writer.save()
    processed_data = output.getvalue()
    b64 = base64.b64encode(processed_data).decode()
    return f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="job_listings.xlsx">Download Excel file</a>'

def detect_language(text):
    try:
        return langdetect.detect(text)
    except:
        return 'en'  # 专专转  转   砖

def analyze_jobs_with_groq(resume, jobs, language):
    api_key = os.getenv('GROQ_API_KEY')
    if not api_key:
        st.error("GROQ API key is missing. Please set it in your .env file.")
        return []

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    prompt = f"""
    Given the following resume and list of job postings, analyze and rank the top 5 jobs that best match the candidate's skills and experience. For each job, provide a brief explanation of why it's a good match.

    The resume is in {language} language. Please provide your response in {language}.

    Resume:
    {resume}

    Job Postings:
    {json.dumps(jobs, ensure_ascii=False)}

    Please provide the results in the following format, using {language}:
    1. Job Title (Company Name)
       Explanation: [Brief explanation of why this job is a good match]

    2. Job Title (Company Name)
       Explanation: [Brief explanation of why this job is a good match]

    ... and so on for the top 5 matches.
    """

    data = {
        "model": "mixtral-8x7b-32768",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.5,
        "max_tokens": 1000
    }

    response = requests.post("https://api.groq.com/openai/v1/chat/completions", headers=headers, json=data)
    
    if response.status_code == 200:
        return response.json()['choices'][0]['message']['content']
    else:
        st.error(f"Error from Groq API: {response.text}")
        return []
    
def main():
    st.set_page_config(layout="wide", page_title="住 砖专转 拽爪注", page_icon="")

    st.markdown("""
    <style>
    @import url('https://fonts.googleapis.com/css2?family=Heebo:wght@400;700&display=swap');
    
    body {
        direction: rtl;
        text-align: right;
        font-family: 'Heebo', sans-serif;
    }
    .stButton>button {
        width: 100%;
    }
    .stSelectbox>div>div>select {
        direction: rtl;
    }
    </style>
    """, unsafe_allow_html=True)

    st.title("住 砖专转 拽爪注")

    selected_agents = st.multiselect("专 住", [agent['name'] for agent in agents], default=[agent['name'] for agent in agents])
    prompt = st.text_input("住 转 驻砖", value=" 砖拽 专转")
    
    uploaded_file = st.file_uploader("注 拽专转  (PDF  Word)", type=["pdf", "docx"])
    
    # 住驻转 驻砖专转 专转 砖驻
    language_options = {
        "注专转": "he",
        "转": "en",
        "专住转": "ru",
        "注专转": "ar",
        "爪专驻转转": "fr"
    }
    selected_language = st.selectbox("专 转 砖驻 砖 转 专爪 拽 转砖", options=list(language_options.keys()))

    if st.button("驻砖 砖专转 砖转转 拽专转  砖"):
        if not selected_agents:
            st.warning(" 专 驻转 住 ")
            return

        progress_bar = st.progress(0)
        status_text = st.empty()

        all_jobs = []
        for i, agent_name in enumerate(selected_agents):
            agent = next((a for a in agents if a['name'] == agent_name), None)
            if agent:
                status_text.text(f"驻砖 砖专转 -{agent_name}...")
                html_content = request_url(agent, prompt=prompt, page=1)
                
                if html_content:
                    extract_function = globals()[f"extract_jobs_{agent['name']}"]
                    extracted_jobs = extract_function(html_content, agent['name'])
                    all_jobs.extend(extracted_jobs)
                
                progress_bar.progress((i + 1) / len(selected_agents))

        if not all_jobs:
            st.warning(" 爪 砖专转 转转")
            return

        status_text.text("爪专 拽抓 Excel...")
        wb = create_excel_from_json(all_jobs)
        
        excel_file = io.BytesIO()
        wb.save(excel_file)
        excel_file.seek(0)

        b64 = base64.b64encode(excel_file.read()).decode()
        href = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="专砖转_砖专转.xlsx">抓  专转 拽抓 Excel</a>'
        st.markdown(href, unsafe_allow_html=True)

        status_text.text(f"爪 {len(all_jobs)} 砖专转 住 .")
        progress_bar.progress(100)

        try:
            df = pd.DataFrame(all_jobs)
            st.write("转爪转 驻砖:")
            st.dataframe(df)

            if uploaded_file is not None:
                resume_content = load_resume(uploaded_file)
                if resume_content:
                    status_text.text("转 转 拽专转 ...")
                    # 砖砖 砖驻 砖专
                    matching_results = analyze_jobs_with_groq(resume_content, all_jobs, language_options[selected_language])
                    st.subheader("砖专转 转转 转专 拽专转  砖:")
                    st.write(matching_results)
            
        except Exception as e:
            st.error(f"专注 砖 爪专转 转 转: {str(e)}")
            st.write("Debug: 转 all_jobs:")
            st.write(all_jobs)

if __name__ == "__main__":
    main()